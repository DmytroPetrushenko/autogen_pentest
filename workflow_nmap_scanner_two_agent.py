from autogen import ConversableAgent, register_function

from llm.lm_studio import start_llm_in_studio
from llm.open_ai import start_llm_openai
from tools.nmap_tool import nmap_scanner
from tools.write_to_file import write_to_file



# Let's first define the assistant agent that suggests tool calls.
assistant = ConversableAgent(
    name="Assistant",
    system_message="You are a helpful AI assistant. "
    "You can help with simple nmap scan. You have two tools: 1) nmap_scanner and 2) write_to_file"
    "If tools will be fit, only use it. Please will write your decision and description!"
    "Return 'TERMINATE' when the task is done.",
    llm_config=start_llm_openai()
)

# The user proxy agent is used for interacting with the assistant agent
# and executes tool calls.
user_proxy = ConversableAgent(
    name="User",
    llm_config=False,
    is_termination_msg=lambda msg: msg.get("content") is not None and "TERMINATE" in msg["content"],
    human_input_mode="NEVER",
)

# Register the tool signature with the assistant agent.
assistant.register_for_llm(name="nmap_scanner", description="A simple nmap scanner")(nmap_scanner)
assistant.register_for_llm(name="write_to_file", description="This function can write a string to a file")(write_to_file)

# Register the tool function with the user proxy agent.
user_proxy.register_for_execution(name="nmap_scanner")(nmap_scanner)
user_proxy.register_for_execution(name="write_to_file")(write_to_file)

# # Register the calculator function .
# register_function(
#     calculator,
#     caller=assistant,  # The assistant agent can suggest calls to the calculator.
#     executor=user_proxy,  # The user proxy agent can execute the calculator calls.
#     name="calculator",  # By default, the function name is used as the tool name.
#     description="A simple calculator",  # A description of the tool.
# )

chat_result = user_proxy.initiate_chat(
    assistant,
    message="scan 63.251.228.0 only these ports: 0-10 and use nmap_scanner for it!"
)

