# Mistral Agent Reporter
import configparser
import math
import os
from autogen import *


from agents.for_group_chat.executor_agents import ExecuteInDocker, ExecutorAgent
from agents.for_group_chat.reporter_agents import ReporterAgent
from llm.lm_studio import start_llm_in_studio
from tools.for_group_chat.common_tools import read_json_from_file
from utils.groupchat_utils import registration_tools_in_agents

config_list = config_list_from_json(
    "../OAI_CONFIG_LIST",
    file_location=os.getcwd(),
    filter_dict={
        "model": ["gpt-4-turbo"],
    }
)


config_path = os.path.join(os.getcwd(), "config.ini")
config = configparser.ConfigParser()
config.read(config_path)

config_list_mistral = [
    {
        # Choose your model name.
        "model": "mistral-large-latest",
        "base_url": "https://api.mistral.ai/v1",
        # You need to provide your API key here.
        "api_key": config["API_KEY"]["MISTRAL_API_KEY"],
    }
]

gpt_config_1 = {"config_list": config_list, "cache_seed": 8888}
gpt_config_2 = {"config_list": config_list, "cache_seed": 8888888}
gpt_config_3 = {"config_list": config_list, "cache_seed": 888888888}

mistral_config = start_llm_in_studio(
    model_api="TheBloke/openbuddy-mixtral-8x7b-v15.1-GGUF/openbuddy-mixtral-8x7b-v15.1.Q2_K.gguf",
    base_url="http://localhost:1234/v1"
)

# reader_agent = AssistantAgent(
#     name="reader_agent",
#     description="You are Reader Agent.",
#     system_message="Goals: Read a file by your tools",
#     llm_config=gpt_config_1
# )


reporter_agent = ReporterAgent(
    name="reporter_agent",
    system_message='GOALS: Analyze the data and create a security report!',
    llm_config={"config_list": config_list_mistral},
    max_consecutive_auto_reply=1
)

init_agent = UserProxyAgent(
    name="init_agent",
    llm_config=False,
    human_input_mode="NEVER",
    code_execution_config=False,
)

execute_in_docker = ExecuteInDocker(image="nmap_python_image:latest", container_name="python_with_nmap_container")
executor_agent = ExecutorAgent(code_execution_config={"executor": execute_in_docker})

# registration_tools_in_agents(reader_agent, "register_for_llm", "common_tools")
# registration_tools_in_agents(executor_agent, "register_for_execution", "common_tools")

# graph_dict = {init_agent: [reader_agent],
#               reader_agent: [executor_agent],
#               executor_agent: [reporter_agent],
#               reporter_agent: [executor_agent]}

file_path = f"{os.getcwd()}/resources/scan_results/result_nmap_scan_2024-04-15_16-29-52.txt"
data = read_json_from_file(file_path)




# tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
# tokens = tokenizer.tokenize(data)
#
# max_tokens_per_message = 2000
# max_tokens: int = len(tokens)
# max_messages: int = math.floor(max_tokens / max_tokens_per_message)
#
#
# context_handling = transform_messages.TransformMessages(
#     transforms=[
#         transforms.MessageHistoryLimiter(max_messages=max_messages),
#         transforms.MessageTokenLimiter(max_tokens=max_tokens, max_tokens_per_message=max_tokens_per_message),
#     ]
# )

# group_chat = GroupChat(
#     agents=[init_agent, reader_agent, executor_agent, reporter_agent],
#     messages=[],
#     max_round=20,
#     speaker_selection_method="auto",
#     allowed_or_disallowed_speaker_transitions=graph_dict,
#     allow_repeat_speaker=None,
#     speaker_transitions_type="allowed"
# )
#
# chat_manager = GroupChatManager(
#     groupchat=group_chat,
#     llm_config=gpt_config_2
# )
#
# context_handling.add_to_agent(reporter_agent)





init_agent.initiate_chat(
    reporter_agent,
    message=f"create security report from this data -> : {data}."
)




